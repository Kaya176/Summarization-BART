{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conditional-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dataset import CustumDataset\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increased-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsBase():\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--train_file',\n",
    "                            type=str,\n",
    "                            default='train_original_base.json',\n",
    "                            help='train file')\n",
    "\n",
    "        parser.add_argument('--test_file',\n",
    "                            type=str,\n",
    "                            default='train_original_base.json',\n",
    "                            help='test file')\n",
    "\n",
    "        parser.add_argument('--batch_size',\n",
    "                            type=int,\n",
    "                            default=8,\n",
    "                            help='')\n",
    "        \n",
    "        parser.add_argument('--max_len',\n",
    "                            type=int,\n",
    "                            default=512,\n",
    "                            help='max seq len')\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moved-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KobartSummary(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,train_file,test_file,\n",
    "                max_len = 512,\n",
    "                batch_size = 8,\n",
    "                num_workers = 5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.train_file_path = train_file\n",
    "        self.test_file_path = test_file\n",
    "        \n",
    "        if tokenizer is None:\n",
    "            self.tok = get_kobart_tokenizer()\n",
    "        else:\n",
    "            self.tok = tokenizer\n",
    "            \n",
    "        self.num_workers = num_workers\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "        parents = [parent_parser],add_help = False)\n",
    "        \n",
    "        parser.add_argument(\"--num_workers\",\n",
    "                           type = int,\n",
    "                           default = 5,\n",
    "                           help = 'num of worker for dataloader')\n",
    "        \n",
    "        return parser\n",
    "    \n",
    "    def ready2data(self,stage):\n",
    "        self.train = CustumDataset(self.train_file_path,\n",
    "                                  self.tok,\n",
    "                                  self.max_len,\n",
    "                                  pad_idx = 0)\n",
    "        \n",
    "        self.test = CustumDataset(self.test_file_path,\n",
    "                                 self.tok,\n",
    "                                 self.max_len,\n",
    "                                 pad_idx = 0)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        train = DataLoader(self.train,\n",
    "                          batch_size = self.batch_size,\n",
    "                          num_workers = self.num_workers, shuffle = True)\n",
    "        return train\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \n",
    "        val = DataLoader(self.test,\n",
    "                          batch_size = self.batch_size,\n",
    "                          num_workers = self.num_workers, shuffle = False)\n",
    "        return val\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        test = DataLoader(self.test,\n",
    "                          batch_size = self.batch_size,\n",
    "                          num_workers = self.num_workers, shuffle = False)\n",
    "        return test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "jewish-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(pl.LightningModule):\n",
    "    def __init__(self,hparams,**kwargs) -> None:\n",
    "        super(Base,self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        # add model specific args\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "\n",
    "        parser.add_argument('--batch-size',\n",
    "                            type=int,\n",
    "                            default=14,\n",
    "                            help='batch size for training (default: 96)')\n",
    "\n",
    "        parser.add_argument('--lr',\n",
    "                            type=float,\n",
    "                            default=3e-5,\n",
    "                            help='The initial learning rate')\n",
    "\n",
    "        parser.add_argument('--warmup_ratio',\n",
    "                            type=float,\n",
    "                            default=0.1,\n",
    "                            help='warmup ratio')\n",
    "\n",
    "        parser.add_argument('--model_path',\n",
    "                            type=str,\n",
    "                            default=None,\n",
    "                            help='kobart model path')\n",
    "        return parser\n",
    "\n",
    "    \n",
    "    def configure_optimizer(self):\n",
    "        #prepare optimizer\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias','LayerNorm.bias','LayerNorm.weight']\n",
    "        #무슨 역할인지 잘 모르겠음.\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\"params\" : [p for n,p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay' : 0.01},\n",
    "            {\"params\" : [p for n,p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay' : 0.0}\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                         lr = self.hparams.lr, correct_bias = False)\n",
    "        \n",
    "        num_workers = self.hparams.num_workers\n",
    "        \n",
    "        data_len = len(self.train_dataloader().dataset)\n",
    "        logging.info(f'number of workers {num_workers}, data length {data_len}')\n",
    "        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)\n",
    "        logging.info(f'num_train_steps : {num_train_steps}')\n",
    "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
    "        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "        lr_scheduler = {'scheduler': scheduler, \n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer],[lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "common-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBartConditionalGeneration(Base):\n",
    "    \n",
    "    def __init__(self,hparams,**kwargs):\n",
    "        super(KoBartConditionalGeneration,self).__init__(hparams,**kwargs)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model())\n",
    "        self.model.train()\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = \"</s>\"\n",
    "        self.pad_token_id = 0\n",
    "        self.tokenizer = get_kobart_tokenizer()\n",
    "        \n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \n",
    "        attention_mask = inputs['encoder_input_ids'].ne(self.pad_token_id).float()\n",
    "        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()\n",
    "        \n",
    "        return self.model(input_ids = inputs[\"encoder_input_ids\"],\n",
    "                         attention_mask = attention_mask,\n",
    "                         decoder_input_ids = inputs['decoder_input_ids'],\n",
    "                          decorder_attention_mask = decoder_attention_mask,\n",
    "                          labels = inputs['label_ids'],return_dict = True)\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        out = self(batch)\n",
    "        loss = out.loss\n",
    "        self.log('train_loss',loss,prog_bar = True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        out = self(batch)\n",
    "        loss = out['loss']\n",
    "        return (loss)\n",
    "    \n",
    "    def validation_epoch_end(self,outputs):\n",
    "        losses = []\n",
    "        for loss in outputs:\n",
    "            losses.append(loss)\n",
    "        \n",
    "        self.log(\"val_loss\",torch.stack(losses).mean(),prog_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "contemporary-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='KoBART Summarization')\n",
    "\n",
    "parser.add_argument('--checkpoint_path',\n",
    "                    type=str,\n",
    "                    help='checkpoint path')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rural-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Base.add_model_specific_args(parser)\n",
    "parser = ArgsBase.add_model_specific_args(parser)\n",
    "parser = KobartSummary.add_model_specific_args(parser)\n",
    "parser = pl.Trainer.add_argparse_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "chinese-cambridge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--checkpoint_path CHECKPOINT_PATH] [--batch-size BATCH_SIZE] [--lr LR] [--warmup_ratio WARMUP_RATIO] [--model_path MODEL_PATH] [--train_file TRAIN_FILE]\n",
      "                             [--test_file TEST_FILE] [--batch_size BATCH_SIZE] [--max_len MAX_LEN] [--num_workers NUM_WORKERS] [--logger [LOGGER]]\n",
      "                             [--checkpoint_callback [CHECKPOINT_CALLBACK]] [--default_root_dir DEFAULT_ROOT_DIR] [--gradient_clip_val GRADIENT_CLIP_VAL] [--process_position PROCESS_POSITION]\n",
      "                             [--num_nodes NUM_NODES] [--num_processes NUM_PROCESSES] [--gpus GPUS] [--auto_select_gpus [AUTO_SELECT_GPUS]] [--tpu_cores TPU_CORES]\n",
      "                             [--log_gpu_memory LOG_GPU_MEMORY] [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE] [--overfit_batches OVERFIT_BATCHES] [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                             [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH] [--fast_dev_run [FAST_DEV_RUN]] [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES] [--max_epochs MAX_EPOCHS]\n",
      "                             [--min_epochs MIN_EPOCHS] [--max_steps MAX_STEPS] [--min_steps MIN_STEPS] [--limit_train_batches LIMIT_TRAIN_BATCHES] [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                             [--limit_test_batches LIMIT_TEST_BATCHES] [--limit_predict_batches LIMIT_PREDICT_BATCHES] [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                             [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS] [--log_every_n_steps LOG_EVERY_N_STEPS] [--accelerator ACCELERATOR] [--sync_batchnorm [SYNC_BATCHNORM]]\n",
      "                             [--precision PRECISION] [--weights_summary WEIGHTS_SUMMARY] [--weights_save_path WEIGHTS_SAVE_PATH] [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                             [--truncated_bptt_steps TRUNCATED_BPTT_STEPS] [--resume_from_checkpoint RESUME_FROM_CHECKPOINT] [--profiler [PROFILER]] [--benchmark [BENCHMARK]]\n",
      "                             [--deterministic [DETERMINISTIC]] [--reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]] [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                             [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]] [--terminate_on_nan [TERMINATE_ON_NAN]] [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                             [--prepare_data_per_node [PREPARE_DATA_PER_NODE]] [--plugins PLUGINS] [--amp_backend AMP_BACKEND] [--amp_level AMP_LEVEL]\n",
      "                             [--distributed_backend DISTRIBUTED_BACKEND] [--automatic_optimization [AUTOMATIC_OPTIMIZATION]] [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                             [--enable_pl_optimizer [ENABLE_PL_OPTIMIZER]] [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE] [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-7688238f-eb50-4e2b-87ac-8323344b2bf6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "joint-stock",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6f8709caf365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKoBartConditionalGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "model = KoBartConditionalGeneration(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-nylon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerical-motivation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/projects/DACON/TextSummarization_gas\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
