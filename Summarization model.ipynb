{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "treated-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#korean tokenizer\n",
    "from kobart import get_kobart_tokenizer\n",
    "from transformers import BartTokenizer #안쓸수도 있음.\n",
    "#kobart model\n",
    "from kobart import get_pytorch_kobart_model\n",
    "from transformers import BartModel\n",
    "#other library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "#my file\n",
    "from load_data import load_data,simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supposed-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "absolute-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "athletic-craft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "judicial-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#데이터는 약 6만개\n",
    "sentence,abst = load_data('train_original.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "genetic-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({\"sentence\" : sentence,\"abstract\" : abst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "intense-andrew",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63067"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "domestic-drawing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    이명박 대통령이 어제 30대 그룹 총수를 모아놓고 \"시대적 요구는 역시 총수가 앞장...\n",
       "abstract    이명박 대통령은 어제 30대 그룹 총수를 모아놓고 시대적 요구는 역시 총수가 앞장서...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-rhythm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이명박 대통령이 어제 30대 그룹 총수를 모아놓고 \"시대적 요구는 역시 총수가 앞장서야 한다. 이미 상당한 변화의 조짐이 있다는 것을 고맙게 생각한다. 총수들께서 직접 관심을 가져주시면 빨리 전파돼 긍정적인 평가를 받을 수 있다고 본다\"고 말했다.언뜻 보아 무슨 말인지 불분명하나 이 대통령이 지난 8ㆍ15 연설 후 정몽준 의원, 정몽구 현대차 회장이 각각 2000억원과 5000억원을 기부한 사실과 \\'공생발전\\'이란 화두를 연결하면 금방 짐작이 간다.다른 그룹 총수들도 좀 나서라고 은근히 떠민 것이다.이 대통령은 기부에 대한 후속 선언이 나오지 않은 탓인지 총수들의 사회공헌 방안에 불만을 표시했다는 후문이다.최근 미국 프랑스 벨기에 등에서 부유세가 거론되고 독일조차 2년간 한시적으로 5%의 자산세를 거둬 약 155조원을 마련하자는 논의가 있었다.이런 흐름에 한국만 동떨어져 있기는 어려운 게 글로벌 시대의 특징이다.항간에는 이번 회동 후 삼성을 비롯해 몇몇 그룹이 노블레스 오블리주 방안을 준비하고 있다는 말이 나도는데 대통령의 강요나 포퓰리즘에 의한 압박보다 자발적 문화로 만들어가야 효과가 큰 법이다.그런 면에서 재계에 적절한 방안 마련을 맡기고 정치권이나 여론은 너무 압박하지 말고 시간을 줘야 한다.국가채무 문제로 글로벌 경기 침체 우려가 큰 상황에서 기업들은 \\'생존\\'에 큰 부담을 느끼고 있기 때문이다.이날 전경련에 따르면 30대 그룹은 올해 고용 12만4000명, 투자 114조원 등 \\'선물\\'을 준비했다.세계적인 더블딥이 우려되는 상황에서 공격경영이 어렵겠지만 연초 한 번 발표한 내용을 약간 수정해 내놓은 전경련의 행태는 답답하다.설립 50주년이 됐으면 좀 더 창의적이고 유연하게 바뀔 때도 됐다.허창수 전경련 회장은 \"대기업ㆍ중소기업이 서로 공생하고 발전할 수 있도록 노력하겠다. 기업이 사회적 책임을 다하겠다\"는 원론적인 발언에 그쳐 전경련 특유의 무미건조함을 드러냈다.한편 이건희 삼성전자 회장은 \"중소기업계 협력을 강화해 국제적으로 경쟁력 있는 기업 생태계를 만들어 나가겠다\"고 발언했고, 정몽구 회장은 \"이제 1차 협력업체는 경쟁력을 확보한 만큼 2ㆍ3차 협력업체 지원에 힘쓰겠다\"고 했는데 의미 있는 내용이라고 본다.그대로 실천하면 동반성장 생태계는 한층 강화될 것이다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "focused-lincoln",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이명박 대통령이 어제 30대 그룹 총수를 모아놓고 시대적 요구는 역시 총수가 앞장서야 한다. 이미 상당한 변화의 조짐이 있다는 것을 고맙게 생각한다. 총수들께서 직접 관심을 가져주\n"
     ]
    }
   ],
   "source": [
    "#Just ONE sentence\n",
    "sample = simple_preprocess(sentence[0])\n",
    "#view sample\n",
    "print(sample[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-scratch",
   "metadata": {},
   "source": [
    "구현단계\n",
    "1. Tokenizer  \n",
    "단순히 get_kobart_tokenizer.tokenize()만 사용하면 단어 id가 따로 출력되지는 않음.  \n",
    "구해야 하는 값 : input_ids, attention_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expensive-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amazing-meaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_kobart_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tested-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁이명박',\n",
       " '▁대통령이',\n",
       " '▁어제',\n",
       " '▁30대',\n",
       " '▁그룹',\n",
       " '▁총',\n",
       " '수를',\n",
       " '▁모아',\n",
       " '놓고',\n",
       " '▁시대',\n",
       " '적',\n",
       " '▁요구',\n",
       " '는',\n",
       " '▁역시',\n",
       " '▁총',\n",
       " '수가',\n",
       " '▁앞장',\n",
       " '서야',\n",
       " '▁한다.',\n",
       " '▁이미',\n",
       " '▁상당한',\n",
       " '▁변화의',\n",
       " '▁조짐',\n",
       " '이',\n",
       " '▁있다는',\n",
       " '▁것을',\n",
       " '▁고맙',\n",
       " '게',\n",
       " '▁생각한',\n",
       " '다.',\n",
       " '▁총',\n",
       " '수',\n",
       " '들',\n",
       " '께서',\n",
       " '▁직접',\n",
       " '▁관심을',\n",
       " '▁가져',\n",
       " '주시',\n",
       " '면',\n",
       " '▁빨리',\n",
       " '▁전파',\n",
       " '돼',\n",
       " '▁긍정적인',\n",
       " '▁평가를',\n",
       " '▁받을',\n",
       " '▁수',\n",
       " '▁있다고',\n",
       " '▁본',\n",
       " '다고',\n",
       " '▁말했다.',\n",
       " '언',\n",
       " '뜻',\n",
       " '▁보아',\n",
       " '▁무슨',\n",
       " '▁말',\n",
       " '인지',\n",
       " '▁불',\n",
       " '분',\n",
       " '명',\n",
       " '하나',\n",
       " '▁이',\n",
       " '▁대통령이',\n",
       " '▁지난',\n",
       " '▁8',\n",
       " 'ᆞ',\n",
       " '15',\n",
       " '▁연설',\n",
       " '▁후',\n",
       " '▁정몽',\n",
       " '준',\n",
       " '▁의원',\n",
       " '▁정몽',\n",
       " '구',\n",
       " '▁현대차',\n",
       " '▁회장이',\n",
       " '▁각각',\n",
       " '▁2000',\n",
       " '억원',\n",
       " '과',\n",
       " '▁5000',\n",
       " '억원을',\n",
       " '▁기부',\n",
       " '한',\n",
       " '▁사실',\n",
       " '과',\n",
       " '▁공',\n",
       " '생',\n",
       " '발전',\n",
       " '이란',\n",
       " '▁화',\n",
       " '두를',\n",
       " '▁연결',\n",
       " '하면',\n",
       " '▁금방',\n",
       " '▁짐작',\n",
       " '이',\n",
       " '▁간',\n",
       " '다.',\n",
       " '다른',\n",
       " '▁그룹',\n",
       " '▁총',\n",
       " '수',\n",
       " '들도',\n",
       " '▁좀',\n",
       " '▁나서',\n",
       " '라고',\n",
       " '▁은',\n",
       " '근',\n",
       " '히',\n",
       " '▁떠',\n",
       " '민',\n",
       " '▁것이다.',\n",
       " '이',\n",
       " '▁대통령은',\n",
       " '▁기',\n",
       " '부에',\n",
       " '▁대한',\n",
       " '▁후속',\n",
       " '▁선언',\n",
       " '이',\n",
       " '▁나오지',\n",
       " '▁않은',\n",
       " '▁탓',\n",
       " '인지',\n",
       " '▁총',\n",
       " '수',\n",
       " '들의',\n",
       " '▁사회공헌',\n",
       " '▁방안에',\n",
       " '▁불만을',\n",
       " '▁표시',\n",
       " '했다는',\n",
       " '▁후문',\n",
       " '이다.',\n",
       " '최근',\n",
       " '▁미국',\n",
       " '▁프랑스',\n",
       " '▁벨',\n",
       " '기에',\n",
       " '▁등에서',\n",
       " '▁부유',\n",
       " '세가',\n",
       " '▁거론',\n",
       " '되고',\n",
       " '▁독일',\n",
       " '조차',\n",
       " '▁2년간',\n",
       " '▁한시',\n",
       " '적으로',\n",
       " '▁5',\n",
       " '의',\n",
       " '▁자산',\n",
       " '세를',\n",
       " '▁거',\n",
       " '둬',\n",
       " '▁약',\n",
       " '▁15',\n",
       " '5',\n",
       " '조',\n",
       " '원을',\n",
       " '▁마련',\n",
       " '하자는',\n",
       " '▁논의가',\n",
       " '▁있었다.',\n",
       " '이',\n",
       " '런',\n",
       " '▁흐름',\n",
       " '에',\n",
       " '▁한국',\n",
       " '만',\n",
       " '▁동',\n",
       " '떨',\n",
       " '어져',\n",
       " '▁있',\n",
       " '기는',\n",
       " '▁어려운',\n",
       " '▁게',\n",
       " '▁글로벌',\n",
       " '▁시대의',\n",
       " '▁특징',\n",
       " '이다.',\n",
       " '항',\n",
       " '간',\n",
       " '에는',\n",
       " '▁이번',\n",
       " '▁회',\n",
       " '동',\n",
       " '▁후',\n",
       " '▁삼',\n",
       " '성을',\n",
       " '▁비롯해',\n",
       " '▁몇몇',\n",
       " '▁그룹',\n",
       " '이',\n",
       " '▁노',\n",
       " '블',\n",
       " '레스',\n",
       " '▁오',\n",
       " '블',\n",
       " '리',\n",
       " '주',\n",
       " '▁방안을',\n",
       " '▁준비하고',\n",
       " '▁있다는',\n",
       " '▁말이',\n",
       " '▁나도',\n",
       " '는데',\n",
       " '▁대통령의',\n",
       " '▁강요',\n",
       " '나',\n",
       " '▁포',\n",
       " '퓰',\n",
       " '리즘',\n",
       " '에',\n",
       " '▁의한',\n",
       " '▁압박',\n",
       " '보다',\n",
       " '▁자발',\n",
       " '적',\n",
       " '▁문화',\n",
       " '로',\n",
       " '▁만들어',\n",
       " '가야',\n",
       " '▁효과가',\n",
       " '▁큰',\n",
       " '▁법',\n",
       " '이다.',\n",
       " '그',\n",
       " '런',\n",
       " '▁면에서',\n",
       " '▁재',\n",
       " '계에',\n",
       " '▁적절한',\n",
       " '▁방안',\n",
       " '▁마련을',\n",
       " '▁맡',\n",
       " '기고',\n",
       " '▁정치권',\n",
       " '이나',\n",
       " '▁여론',\n",
       " '은',\n",
       " '▁너무',\n",
       " '▁압박',\n",
       " '하지',\n",
       " '▁말고',\n",
       " '▁시간을',\n",
       " '▁',\n",
       " '줘야',\n",
       " '▁한다.',\n",
       " '국가',\n",
       " '채',\n",
       " '무',\n",
       " '▁문제로',\n",
       " '▁글로벌',\n",
       " '▁경기',\n",
       " '▁침체',\n",
       " '▁우려가',\n",
       " '▁큰',\n",
       " '▁상황에서',\n",
       " '▁기업들은',\n",
       " '▁생존',\n",
       " '에',\n",
       " '▁큰',\n",
       " '▁부담을',\n",
       " '▁느끼고',\n",
       " '▁있기',\n",
       " '▁때문이다.',\n",
       " '이',\n",
       " '날',\n",
       " '▁전경',\n",
       " '련',\n",
       " '에',\n",
       " '▁따르면',\n",
       " '▁30대',\n",
       " '▁그룹',\n",
       " '은',\n",
       " '▁올해',\n",
       " '▁고용',\n",
       " '▁12',\n",
       " '만',\n",
       " '4000',\n",
       " '명',\n",
       " '▁투자',\n",
       " '▁11',\n",
       " '4',\n",
       " '조원',\n",
       " '▁등',\n",
       " '▁선물을',\n",
       " '▁준비',\n",
       " '했다.',\n",
       " '세계',\n",
       " '적인',\n",
       " '▁더블',\n",
       " '딥',\n",
       " '이',\n",
       " '▁우려',\n",
       " '되는',\n",
       " '▁상황에서',\n",
       " '▁공격',\n",
       " '경영',\n",
       " '이',\n",
       " '▁어렵',\n",
       " '겠지만',\n",
       " '▁연초',\n",
       " '▁한',\n",
       " '▁번',\n",
       " '▁발표한',\n",
       " '▁내용을',\n",
       " '▁약간',\n",
       " '▁수정',\n",
       " '해',\n",
       " '▁내놓은',\n",
       " '▁전경',\n",
       " '련의',\n",
       " '▁행태',\n",
       " '는',\n",
       " '▁답답',\n",
       " '하다.',\n",
       " '설',\n",
       " '립',\n",
       " '▁50',\n",
       " '주년',\n",
       " '이',\n",
       " '▁됐',\n",
       " '으면',\n",
       " '▁좀',\n",
       " '▁더',\n",
       " '▁창의',\n",
       " '적이고',\n",
       " '▁유연',\n",
       " '하게',\n",
       " '▁바뀔',\n",
       " '▁때도',\n",
       " '▁됐',\n",
       " '다.',\n",
       " '허',\n",
       " '창',\n",
       " '수',\n",
       " '▁전경',\n",
       " '련',\n",
       " '▁회장은',\n",
       " '▁대기업',\n",
       " 'ᆞ',\n",
       " '중',\n",
       " '소기업',\n",
       " '이',\n",
       " '▁서로',\n",
       " '▁공',\n",
       " '생',\n",
       " '하고',\n",
       " '▁발전',\n",
       " '할',\n",
       " '▁수',\n",
       " '▁있도록',\n",
       " '▁노력',\n",
       " '하겠',\n",
       " '다.',\n",
       " '▁기업이',\n",
       " '▁사회적',\n",
       " '▁책임을',\n",
       " '▁다',\n",
       " '하겠다는',\n",
       " '▁원',\n",
       " '론',\n",
       " '적인',\n",
       " '▁발언에',\n",
       " '▁그',\n",
       " '쳐',\n",
       " '▁전경',\n",
       " '련',\n",
       " '▁특유의',\n",
       " '▁무',\n",
       " '미',\n",
       " '건',\n",
       " '조',\n",
       " '함을',\n",
       " '▁드러',\n",
       " '냈',\n",
       " '다.',\n",
       " '한',\n",
       " '편',\n",
       " '▁이건',\n",
       " '희',\n",
       " '▁삼성전자',\n",
       " '▁회장은',\n",
       " '▁중소기업',\n",
       " '계',\n",
       " '▁협력을',\n",
       " '▁강화해',\n",
       " '▁국제',\n",
       " '적으로',\n",
       " '▁경쟁력',\n",
       " '▁있는',\n",
       " '▁기업',\n",
       " '▁생태',\n",
       " '계를',\n",
       " '▁만들어',\n",
       " '▁나가',\n",
       " '겠다고',\n",
       " '▁발언',\n",
       " '했고',\n",
       " '▁정몽',\n",
       " '구',\n",
       " '▁회장은',\n",
       " '▁이제',\n",
       " '▁1차',\n",
       " '▁협력업체',\n",
       " '는',\n",
       " '▁경쟁력을',\n",
       " '▁확보한',\n",
       " '▁만큼',\n",
       " '▁2',\n",
       " 'ᆞ',\n",
       " '3',\n",
       " '차',\n",
       " '▁협력업체',\n",
       " '▁지원에',\n",
       " '▁힘',\n",
       " '쓰',\n",
       " '겠다고',\n",
       " '▁했는데',\n",
       " '▁의미',\n",
       " '▁있는',\n",
       " '▁내용',\n",
       " '이라고',\n",
       " '▁본',\n",
       " '다.',\n",
       " '그',\n",
       " '대로',\n",
       " '▁실천',\n",
       " '하면',\n",
       " '▁동반성장',\n",
       " '▁생태',\n",
       " '계는',\n",
       " '▁한층',\n",
       " '▁강화',\n",
       " '될',\n",
       " '▁것이다.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secure-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "automated-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#add batch\n",
    "input_ids = torch.tensor(input_ids)\n",
    "input_ids_batch = input_ids.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "after-customs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 438])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "protecting-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_k = torch.tensor([1,2,3,4,5,6,7,8,0,0])\n",
    "seq_k = seq_k.unsqueeze(0)\n",
    "print(seq_k.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "static-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # print(seq_q)\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ethical-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_attn_pad_mask(seq_k,seq_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "shared-turkey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "diverse-bride",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-understanding",
   "metadata": {},
   "source": [
    "function : masked_fill  \n",
    "masked_fill함수는 조건을 만족하는 모든 값들을 다른 값으로 바꿔주는 함수라고 생각할 수 있음.  \n",
    "아래 예시는 mask값이 1이면 원래 score를 -999로 바꾸는 예시."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "earlier-yellow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999],\n",
       "        [    1,     2, -9999, -9999, -9999, -9999, -9999, -9999, -9999, -9999],\n",
       "        [    1,     2,     3, -9999, -9999, -9999, -9999, -9999, -9999, -9999],\n",
       "        [    1,     2,     3,     4, -9999, -9999, -9999, -9999, -9999, -9999],\n",
       "        [    1,     2,     3,     4,     5, -9999, -9999, -9999, -9999, -9999],\n",
       "        [    1,     2,     3,     4,     5,     6, -9999, -9999, -9999, -9999],\n",
       "        [    1,     2,     3,     4,     5,     6,     7, -9999, -9999, -9999],\n",
       "        [    1,     2,     3,     4,     5,     6,     7,     8, -9999, -9999],\n",
       "        [    1,     2,     3,     4,     5,     6,     7,     8,     0, -9999],\n",
       "        [    1,     2,     3,     4,     5,     6,     7,     8,     0,     0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_k.masked_fill(mask == 0,-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "documented-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tril(torch.ones(10,10)).type(torch.BoolTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contained-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "'''\n",
    "dataset을 만들기 위해 구현해야할 것:\n",
    "__init__ / __getitem__ / __len__\n",
    "\n",
    "특히, __getitem__ 은 encoder_input_id, decoder_input_id,label_id 정보가 반드시 필요함.\n",
    "encoder_input_id는 [1,2,3,4,5] 이런식 / 다른 정보도 마찬가지로 토큰화시키면 됨.\n",
    "'''\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustumDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,file_name,tokenizer,max_len,pad_idx=None,ignore_idx = -999):\n",
    "        self.tokenizer = tokenizer #토크나이저 설정\n",
    "        self.max_len = max_len #sentence 최대길이 지정\n",
    "        self.data = load_data(file_name) #데이터 불러오기\n",
    "        self.len = self.data.shape[0] #데이터의 총 갯수\n",
    "        self.pad_idx = pad_idx\n",
    "        self.ignore_idx = ignore_idx\n",
    "        \n",
    "    def _add_padding2data(self,sentence):\n",
    "        '''\n",
    "        입력한 sequence가 max_len보다 작은 경우 padding을 진행해줌으로써 길이를 모두 동일하게 맞춰준다.\n",
    "        반대로 sentence가 max_len보다 크거나 같은 경우, max_len만큼만 쓸 것이므로 잘라준다.\n",
    "        '''\n",
    "        if len(sentence) < self.max_len:\n",
    "            pad_seq = np.array([self.pad_idx] * (self.max_len - len(sentence)))\n",
    "            sentence = np.concatenate([sentence,pad_seq])\n",
    "        else:\n",
    "            sentence = sentence[:self.max_len]\n",
    "        return sentence\n",
    "    \n",
    "    def _add_ignore2data(self,sentence):\n",
    "        '''\n",
    "        label(요약문)에 사용하는 함수로써 padding과 더불어 ignore_index를 추가해줌. 불필요한 길이를 방지하기 위함인거같음.(더 알아보기)\n",
    "        '''\n",
    "        if len(sentence) < self.max_len:\n",
    "            pad_seq = np.array([self.ignore_idx] * (self.max_len - len(sentence)))\n",
    "            sentence = np.concatenate([sentence,pad_seq])\n",
    "        else:\n",
    "            sentence = sentence[:self.max_len]\n",
    "        return sentence\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        #just for one line\n",
    "        line = self.data.iloc[idx]\n",
    "        sentence = line['sentence']\n",
    "        abst = line['abstract']\n",
    "        #encode : encoder input\n",
    "        encoder_input_ids = self.tokenizer.encode_plus(sentence).input_ids\n",
    "        encoder_input_ids = self._add_padding2data(encoder_input_ids)\n",
    "        #encode : label (요약문)\n",
    "        label_ids = self.tokenizer.encode_plus(abst).input_ids\n",
    "        label_ids.append(self.tokenizer.eos_token_id)\n",
    "        #encode : decoder input\n",
    "        dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self._add_padding2data(dec_input_ids)\n",
    "        #add ignore_idx\n",
    "        label_ids = self._add_ignore2data(label_ids)\n",
    "        return {\"encoder_input_ids\" : encoder_input_ids ,\n",
    "                \"decoder_input_ids\" : dec_input_ids,\n",
    "                'label_ids' : label_ids}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nutritional-diving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "File_name = \"train_original.json\"\n",
    "tokenizer = get_kobart_tokenizer()\n",
    "max_len = 2048\n",
    "pad_idx = 0\n",
    "data = CustumDataset(file_name = File_name,tokenizer = tokenizer, max_len = max_len,pad_idx = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "final-russia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder_input_ids': array([16954, 15727, 14522, ...,     0,     0,     0]),\n",
       " 'decoder_input_ids': array([    1, 15741, 11911, ...,     0,     0,     0]),\n",
       " 'label_ids': array([15741, 11911, 11764, ...,  -999,  -999,  -999])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enclosed-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "standard-lawsuit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bart = BartModel.from_pretrained(get_pytorch_kobart_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "joined-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bart(seq_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "possible-remove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "output = bart(seq_k).last_hidden_state\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "public-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(768,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fitting-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "linear_output = linear(output)\n",
    "print(linear_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-newcastle",
   "metadata": {},
   "source": [
    "bart model의 output : last_hidden_state,past_key_values,encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "religious-genre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.9937,  0.2334,  2.9882,  ...,  0.8528,  1.0862, -0.3879],\n",
       "         [ 3.5808,  0.3890,  3.5731,  ..., -0.8300,  0.7109, -0.7916],\n",
       "         [ 3.8944,  0.4965,  2.8769,  ...,  0.8735,  0.9011, -0.2143],\n",
       "         ...,\n",
       "         [ 2.4328,  0.9183,  1.0239,  ..., -1.0559,  1.7633,  0.1214],\n",
       "         [ 1.6645,  0.3718,  0.0478,  ...,  0.0165,  1.2465,  0.6601],\n",
       "         [ 2.3392,  1.1378,  1.0062,  ...,  1.4361,  0.3571, -0.0564]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "serious-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "hollywood-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bart_generation = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
